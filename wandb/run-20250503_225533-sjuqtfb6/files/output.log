[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 42/198 [1:19:38<7:06:55, 164.20s/it]Traceback (most recent call last):
{'loss': 2.848, 'grad_norm': 27.44721221923828, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.15}
  File "/nas02/Hadi/Model-Selection-IF/alphalora/mola_training_mistral.py", line 330, in <module>
{'eval_loss': 2.1356940269470215, 'eval_runtime': 0.5986, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'epoch': 0.15}
{'loss': 1.1955, 'grad_norm': 16.6695613861084, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.3}
{'eval_loss': 1.3962509632110596, 'eval_runtime': 8.4194, 'eval_samples_per_second': 0.119, 'eval_steps_per_second': 0.119, 'epoch': 0.3}
{'loss': 0.7374, 'grad_norm': 10.070443153381348, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.45}
{'eval_loss': 1.3264824151992798, 'eval_runtime': 8.4019, 'eval_samples_per_second': 0.119, 'eval_steps_per_second': 0.119, 'epoch': 0.45}
{'loss': 0.7452, 'grad_norm': 3.787933588027954, 'learning_rate': 0.00011399999999999999, 'epoch': 0.6}
{'eval_loss': 1.2239465713500977, 'eval_runtime': 8.6868, 'eval_samples_per_second': 0.115, 'eval_steps_per_second': 0.115, 'epoch': 0.6}
    fire.Fire(train)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/nas02/Hadi/Model-Selection-IF/alphalora/mola_training_mistral.py", line 321, in train
    trainer.train()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2268, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 3324, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/accelerate/accelerator.py", line 1964, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7facfc3cb760>
Traceback (most recent call last):
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
