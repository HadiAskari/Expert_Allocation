[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
                                                                                                                                 
{'loss': 3.9157, 'grad_norm': 18.535280227661133, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.13}
                                                                                                                                 
{'eval_loss': 2.1539084911346436, 'eval_runtime': 0.5617, 'eval_samples_per_second': 1.78, 'eval_steps_per_second': 1.78, 'epoch': 0.13}
{'loss': 1.6999, 'grad_norm': 2.569040536880493, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.26}
{'eval_loss': 0.9532952904701233, 'eval_runtime': 0.5631, 'eval_samples_per_second': 1.776, 'eval_steps_per_second': 1.776, 'epoch': 0.26}
{'loss': 1.2579, 'grad_norm': 0.6551085710525513, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.39}
{'eval_loss': 0.7817991971969604, 'eval_runtime': 0.5611, 'eval_samples_per_second': 1.782, 'eval_steps_per_second': 1.782, 'epoch': 0.39}
{'loss': 1.0239, 'grad_norm': 1.5547995567321777, 'learning_rate': 0.00011999999999999999, 'epoch': 0.53}
{'eval_loss': 0.7832721471786499, 'eval_runtime': 0.5595, 'eval_samples_per_second': 1.787, 'eval_steps_per_second': 1.787, 'epoch': 0.53}
{'loss': 1.1015, 'grad_norm': 0.6760088801383972, 'learning_rate': 0.00015, 'epoch': 0.66}
{'eval_loss': 0.7213649749755859, 'eval_runtime': 0.5607, 'eval_samples_per_second': 1.783, 'eval_steps_per_second': 1.783, 'epoch': 0.66}
{'loss': 1.0109, 'grad_norm': 1.178507924079895, 'learning_rate': 0.00017999999999999998, 'epoch': 0.79}
{'eval_loss': 0.6957467198371887, 'eval_runtime': 0.5541, 'eval_samples_per_second': 1.805, 'eval_steps_per_second': 1.805, 'epoch': 0.79}
{'loss': 1.0281, 'grad_norm': 0.47555068135261536, 'learning_rate': 0.00020999999999999998, 'epoch': 0.92}
{'eval_loss': 0.6886683702468872, 'eval_runtime': 0.5785, 'eval_samples_per_second': 1.729, 'eval_steps_per_second': 1.729, 'epoch': 0.92}
current_epoch:  1
{'loss': 0.9944, 'grad_norm': 35.440643310546875, 'learning_rate': 0.00023999999999999998, 'epoch': 1.05}
{'eval_loss': 0.7066129446029663, 'eval_runtime': 0.5923, 'eval_samples_per_second': 1.688, 'eval_steps_per_second': 1.688, 'epoch': 1.05}
{'loss': 0.9222, 'grad_norm': 0.5594012141227722, 'learning_rate': 0.00027, 'epoch': 1.18}
{'eval_loss': 0.7243509292602539, 'eval_runtime': 0.5894, 'eval_samples_per_second': 1.697, 'eval_steps_per_second': 1.697, 'epoch': 1.18}
{'loss': 0.9563, 'grad_norm': 1.2617335319519043, 'learning_rate': 0.0003, 'epoch': 1.31}
{'eval_loss': 0.7436773777008057, 'eval_runtime': 0.5963, 'eval_samples_per_second': 1.677, 'eval_steps_per_second': 1.677, 'epoch': 1.31}
{'loss': 0.9024, 'grad_norm': 0.6346015334129333, 'learning_rate': 0.00027656249999999995, 'epoch': 1.44}
{'eval_loss': 0.7260460257530212, 'eval_runtime': 0.5731, 'eval_samples_per_second': 1.745, 'eval_steps_per_second': 1.745, 'epoch': 1.44}
{'loss': 0.9777, 'grad_norm': 0.807341992855072, 'learning_rate': 0.000253125, 'epoch': 1.58}
{'eval_loss': 0.6888477206230164, 'eval_runtime': 0.597, 'eval_samples_per_second': 1.675, 'eval_steps_per_second': 1.675, 'epoch': 1.58}
{'loss': 0.8731, 'grad_norm': 0.6251989006996155, 'learning_rate': 0.0002296875, 'epoch': 1.71}
{'eval_loss': 0.7271577715873718, 'eval_runtime': 0.5954, 'eval_samples_per_second': 1.679, 'eval_steps_per_second': 1.679, 'epoch': 1.71}
{'loss': 0.9879, 'grad_norm': 1.1572805643081665, 'learning_rate': 0.00020624999999999997, 'epoch': 1.84}
{'eval_loss': 0.7154611349105835, 'eval_runtime': 0.5895, 'eval_samples_per_second': 1.696, 'eval_steps_per_second': 1.696, 'epoch': 1.84}
{'loss': 0.8449, 'grad_norm': 0.6016737818717957, 'learning_rate': 0.00018281249999999998, 'epoch': 1.97}
{'eval_loss': 0.7108002305030823, 'eval_runtime': 0.5804, 'eval_samples_per_second': 1.723, 'eval_steps_per_second': 1.723, 'epoch': 1.97}
current_epoch:  2
{'loss': 0.868, 'grad_norm': 1.265578269958496, 'learning_rate': 0.00015937499999999998, 'epoch': 2.1}
{'eval_loss': 0.6547125577926636, 'eval_runtime': 0.5997, 'eval_samples_per_second': 1.667, 'eval_steps_per_second': 1.667, 'epoch': 2.1}
{'loss': 0.6289, 'grad_norm': 0.73380446434021, 'learning_rate': 0.0001359375, 'epoch': 2.23}
{'eval_loss': 0.7443662285804749, 'eval_runtime': 0.5964, 'eval_samples_per_second': 1.677, 'eval_steps_per_second': 1.677, 'epoch': 2.23}
{'loss': 0.841, 'grad_norm': 0.8705790638923645, 'learning_rate': 0.0001125, 'epoch': 2.36}
{'eval_loss': 0.7150718569755554, 'eval_runtime': 0.5931, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 1.686, 'epoch': 2.36}
{'loss': 0.6094, 'grad_norm': 0.7264792323112488, 'learning_rate': 8.906249999999999e-05, 'epoch': 2.5}
{'eval_loss': 0.7306832671165466, 'eval_runtime': 0.5667, 'eval_samples_per_second': 1.765, 'eval_steps_per_second': 1.765, 'epoch': 2.5}
{'loss': 0.8558, 'grad_norm': 0.7437551021575928, 'learning_rate': 6.5625e-05, 'epoch': 2.63}
{'eval_loss': 0.7051047086715698, 'eval_runtime': 0.5976, 'eval_samples_per_second': 1.673, 'eval_steps_per_second': 1.673, 'epoch': 2.63}
{'loss': 0.6489, 'grad_norm': 1.0042376518249512, 'learning_rate': 4.2187499999999995e-05, 'epoch': 2.76}
{'eval_loss': 0.7331945300102234, 'eval_runtime': 0.5895, 'eval_samples_per_second': 1.696, 'eval_steps_per_second': 1.696, 'epoch': 2.76}
{'loss': 0.7672, 'grad_norm': 0.6043204665184021, 'learning_rate': 1.875e-05, 'epoch': 2.89}
{'eval_loss': 0.7261723875999451, 'eval_runtime': 0.5499, 'eval_samples_per_second': 1.819, 'eval_steps_per_second': 1.819, 'epoch': 2.89}
    fire.Fire(train)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/nas02/Hadi/Model-Selection-IF/alphalora/mola_training_gemma.py", line 321, in train
    trainer.train()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2345, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2796, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2875, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 3429, in save_model
    self._save(output_dir)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 3494, in _save
    safetensors.torch.save_file(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/safetensors/torch.py", line 284, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/safetensors/torch.py", line 480, in _flatten
    raise RuntimeError(
RuntimeError:
            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.model.embed_tokens.weight'}].
            A potential way to correctly save your model is to use `save_model`.
            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors

