[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [41:17<00:00, 30.17s/it]Traceback (most recent call last):
{'loss': 3.5874, 'grad_norm': 16.60918617248535, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.35}
  File "/nas02/Hadi/Model-Selection-IF/alphalora/mola_training_gemma.py", line 330, in <module>                                  
{'eval_loss': 2.4320147037506104, 'eval_runtime': 0.6825, 'eval_samples_per_second': 1.465, 'eval_steps_per_second': 1.465, 'epoch': 0.35}
{'loss': 1.5749, 'grad_norm': 1.4492788314819336, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.7}
{'eval_loss': 1.2263779640197754, 'eval_runtime': 0.6687, 'eval_samples_per_second': 1.495, 'eval_steps_per_second': 1.495, 'epoch': 0.7}
current_epoch:  1
{'loss': 1.0044, 'grad_norm': 1.000399112701416, 'learning_rate': 8.999999999999999e-05, 'epoch': 1.05}
{'eval_loss': 1.0117071866989136, 'eval_runtime': 0.6741, 'eval_samples_per_second': 1.483, 'eval_steps_per_second': 1.483, 'epoch': 1.05}
{'loss': 0.9149, 'grad_norm': 0.48684537410736084, 'learning_rate': 0.00011999999999999999, 'epoch': 1.39}
{'eval_loss': 0.9573752880096436, 'eval_runtime': 0.6634, 'eval_samples_per_second': 1.507, 'eval_steps_per_second': 1.507, 'epoch': 1.39}
{'loss': 0.8725, 'grad_norm': 0.44707173109054565, 'learning_rate': 0.00015, 'epoch': 1.74}
{'eval_loss': 0.9906299710273743, 'eval_runtime': 0.6847, 'eval_samples_per_second': 1.461, 'eval_steps_per_second': 1.461, 'epoch': 1.74}
current_epoch:  2
{'loss': 0.8484, 'grad_norm': 0.41688698530197144, 'learning_rate': 0.00017999999999999998, 'epoch': 2.09}
{'eval_loss': 0.9355423450469971, 'eval_runtime': 0.7232, 'eval_samples_per_second': 1.383, 'eval_steps_per_second': 1.383, 'epoch': 2.09}
{'loss': 0.7354, 'grad_norm': 0.6663349270820618, 'learning_rate': 0.00020999999999999998, 'epoch': 2.44}
{'eval_loss': 0.787839949131012, 'eval_runtime': 0.7099, 'eval_samples_per_second': 1.409, 'eval_steps_per_second': 1.409, 'epoch': 2.44}
{'loss': 0.7198, 'grad_norm': 0.8820547461509705, 'learning_rate': 0.00023999999999999998, 'epoch': 2.79}
{'eval_loss': 0.8485630750656128, 'eval_runtime': 0.6873, 'eval_samples_per_second': 1.455, 'eval_steps_per_second': 1.455, 'epoch': 2.79}
    fire.Fire(train)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/nas02/Hadi/Model-Selection-IF/alphalora/mola_training_gemma.py", line 321, in train
    trainer.train()
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2345, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2796, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 2875, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 3429, in save_model
    self._save(output_dir)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/transformers/trainer.py", line 3494, in _save
    safetensors.torch.save_file(
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/safetensors/torch.py", line 284, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
  File "/home/haskari/miniconda3/envs/alphalora2/lib/python3.10/site-packages/safetensors/torch.py", line 480, in _flatten
    raise RuntimeError(
RuntimeError:
            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.model.embed_tokens.weight'}].
            A potential way to correctly save your model is to use `save_model`.
            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors

